{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single card inference validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextGenerationPipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/zangyuchen/trlx/tmp/ckpts/best_checkpoint were not used when initializing GPT2LMHeadModel: ['v_head.2.bias', 'v_head.2.weight', 'v_head.0.bias', 'v_head.0.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 加载新模型checkpoint的时候需要把原模型的config文件拷贝到checkpoint目录下\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data-ai/model/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data/zangyuchen/trlx/tmp/ckpts/best_checkpoint\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(\"/data-ai/model/gpt2\").to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new result:the movie is very hard.\n",
      "\n",
      "When it came time for our interview with Jim, he said he was in no rush — because from what we\n",
      "old result:the movie is the story of \"Oscar\" Jackson. In The Lion King, he plays Prince of Persia, played by his long haired,\n"
     ]
    }
   ],
   "source": [
    "do_sample = True\n",
    "prompt = \"the movie is\"\n",
    "max_length = 30\n",
    "new_pipline = TextGenerationPipeline(model = model, tokenizer = tokenizer, device = 0,do_sample=do_sample, max_length=max_length)\n",
    "old_pipline = TextGenerationPipeline(model = original_model, tokenizer = tokenizer, device = 1,do_sample=do_sample,max_length=max_length)\n",
    "print(f\"new result:{new_pipline(prompt)[0]['generated_text']}\")\n",
    "print(f\"old result:{old_pipline(prompt)[0]['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuchen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
